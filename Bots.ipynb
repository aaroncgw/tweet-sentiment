{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modules.tweet_data import read_raw_data\n",
    "from modules.spacy import spacy_twitter_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading data\n"
     ]
    }
   ],
   "source": [
    "tweet_df = read_raw_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "REGEX_DICT = {'link': \"http\\S+\",\n",
    "              'piclink': \"pic.twitter.com\\S+\",\n",
    "              'hashtag': \"\\B#\\w*[a-zA-Z]+\\w*|#\\w*[a-zA-Z]+\\w*\",\n",
    "              'email': \"[a-zA-Z0-9+._-]+@[a-zA-Z0-9._-]+\\.[a-zA-Z0-9_-]+\",\n",
    "              'add': \"\\s([@][\\w_-]+)|[@][\\w_-]+\"}\n",
    "\n",
    "for key, value in REGEX_DICT.items():\n",
    "    print('Filtering', key)\n",
    "    pearkes.tweet = pearkes.tweet.apply(lambda tweet: re.sub(value, key.upper(), tweet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pearkes = tweet_df[tweet_df.handle=='pearkes']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('pearkes.txt', 'w') as filehandle:\n",
    "    for listitem in pearkes.tweet.to_list():\n",
    "        filehandle.write('%s\\n' % listitem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "07/28/2020 12:39:38 - INFO - transformers.training_args -   PyTorch: setting up devices\n",
      "07/28/2020 12:39:38 - WARNING - __main__ -   Process rank: -1, device: cpu, n_gpu: 0, distributed training: False, 16-bits training: False\n",
      "07/28/2020 12:39:38 - INFO - __main__ -   Training/evaluation parameters TrainingArguments(output_dir='here/', overwrite_output_dir=False, do_train=True, do_eval=True, do_predict=False, evaluate_during_training=False, per_device_train_batch_size=8, per_device_eval_batch_size=8, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=1, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=3.0, max_steps=-1, warmup_steps=0, logging_dir='runs/Jul28_12-39-38_Eduardos-MacBook-Pro.local', logging_first_step=False, logging_steps=500, save_steps=500, save_total_limit=None, no_cuda=False, seed=42, fp16=False, fp16_opt_level='O1', local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=False, dataloader_drop_last=False, eval_steps=1000, past_index=-1)\n",
      "07/28/2020 12:39:39 - INFO - transformers.configuration_utils -   loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-config.json from cache at /Users/lalopey/.cache/torch/transformers/4be02c5697d91738003fb1685c9872f284166aa32e061576bbe6aaeb95649fcf.db13c9bc9c7bdd738ec89e069621d88e05dc670366092d809a9cbcac6798e24e\n",
      "07/28/2020 12:39:39 - INFO - transformers.configuration_utils -   Model config GPT2Config {\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 1024,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n",
      "07/28/2020 12:39:40 - INFO - transformers.configuration_utils -   loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-config.json from cache at /Users/lalopey/.cache/torch/transformers/4be02c5697d91738003fb1685c9872f284166aa32e061576bbe6aaeb95649fcf.db13c9bc9c7bdd738ec89e069621d88e05dc670366092d809a9cbcac6798e24e\n",
      "07/28/2020 12:39:40 - INFO - transformers.configuration_utils -   Model config GPT2Config {\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 1024,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n",
      "07/28/2020 12:39:42 - INFO - transformers.tokenization_utils_base -   loading file https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-vocab.json from cache at /Users/lalopey/.cache/torch/transformers/f2808208f9bec2320371a9f5f891c184ae0b674ef866b79c58177067d15732dd.1512018be4ba4e8726e41b9145129dc30651ea4fec86aa61f4b9f40bf94eac71\n",
      "07/28/2020 12:39:42 - INFO - transformers.tokenization_utils_base -   loading file https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-merges.txt from cache at /Users/lalopey/.cache/torch/transformers/d629f792e430b3c76a1291bb2766b0a047e36fae0588f9dbc1ae51decdff691b.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda\n",
      "/Users/lalopey/opt/anaconda3/envs/tweet-sentiment/lib/python3.7/site-packages/transformers/modeling_auto.py:798: FutureWarning: The class `AutoModelWithLMHead` is deprecated and will be removed in a future version. Please use `AutoModelForCausalLM` for causal language models, `AutoModelForMaskedLM` for masked language models and `AutoModelForSeq2SeqLM` for encoder-decoder models.\n",
      "  FutureWarning,\n",
      "07/28/2020 12:39:42 - INFO - transformers.modeling_utils -   loading weights file https://cdn.huggingface.co/gpt2-pytorch_model.bin from cache at /Users/lalopey/.cache/torch/transformers/d71fd633e58263bd5e91dd3bde9f658bafd81e11ece622be6a3c2e4d42d8fd89.778cf36f5c4e5d94c8cd9cefcf2a580c8643570eb327f0d4a1f007fab2acbdf1\n",
      "07/28/2020 12:39:44 - INFO - transformers.modeling_utils -   All model checkpoint weights were used when initializing GPT2LMHeadModel.\n",
      "\n",
      "07/28/2020 12:39:44 - WARNING - transformers.modeling_utils -   Some weights of GPT2LMHeadModel were not initialized from the model checkpoint at gpt2 and are newly initialized: ['h.0.attn.masked_bias', 'h.1.attn.masked_bias', 'h.2.attn.masked_bias', 'h.3.attn.masked_bias', 'h.4.attn.masked_bias', 'h.5.attn.masked_bias', 'h.6.attn.masked_bias', 'h.7.attn.masked_bias', 'h.8.attn.masked_bias', 'h.9.attn.masked_bias', 'h.10.attn.masked_bias', 'h.11.attn.masked_bias', 'lm_head.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "07/28/2020 12:39:44 - INFO - filelock -   Lock 140546772877264 acquired on cached_lm_GPT2Tokenizer_1024_listfile.txt.lock\n",
      "07/28/2020 12:39:44 - INFO - transformers.data.datasets.language_modeling -   Loading features from cached file cached_lm_GPT2Tokenizer_1024_listfile.txt [took 0.201 s]\n",
      "07/28/2020 12:39:44 - INFO - filelock -   Lock 140546772877264 released on cached_lm_GPT2Tokenizer_1024_listfile.txt.lock\n",
      "07/28/2020 12:39:44 - INFO - filelock -   Lock 140547308843408 acquired on cached_lm_GPT2Tokenizer_1024_listfile.txt.lock\n",
      "07/28/2020 12:39:45 - INFO - transformers.data.datasets.language_modeling -   Loading features from cached file cached_lm_GPT2Tokenizer_1024_listfile.txt [took 0.139 s]\n",
      "07/28/2020 12:39:45 - INFO - filelock -   Lock 140547308843408 released on cached_lm_GPT2Tokenizer_1024_listfile.txt.lock\n",
      "07/28/2020 12:39:45 - WARNING - transformers.trainer -   You are instantiating a Trainer but Tensorboard is not installed. You should consider installing it.\n",
      "07/28/2020 12:39:45 - INFO - transformers.trainer -   You are instantiating a Trainer but W&B is not installed. To use wandb logging, run `pip install wandb; wandb login` see https://docs.wandb.com/huggingface.\n",
      "07/28/2020 12:39:45 - INFO - transformers.trainer -   ***** Running training *****\n",
      "07/28/2020 12:39:45 - INFO - transformers.trainer -     Num examples = 3867\n",
      "07/28/2020 12:39:45 - INFO - transformers.trainer -     Num Epochs = 3\n",
      "07/28/2020 12:39:45 - INFO - transformers.trainer -     Instantaneous batch size per device = 8\n",
      "07/28/2020 12:39:45 - INFO - transformers.trainer -     Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "07/28/2020 12:39:45 - INFO - transformers.trainer -     Gradient Accumulation steps = 1\n",
      "07/28/2020 12:39:45 - INFO - transformers.trainer -     Total optimization steps = 1452\n",
      "Epoch:   0%|                                              | 0/3 [00:00<?, ?it/s]\n",
      "Iteration:   0%|                                        | 0/484 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch:   0%|                                              | 0/3 [00:00<?, ?it/s]\n",
      "Traceback (most recent call last):\n",
      "  File \"run_language_modeling.py\", line 297, in <module>\n",
      "    main()\n",
      "  File \"run_language_modeling.py\", line 261, in main\n",
      "    trainer.train(model_path=model_path)\n",
      "  File \"/Users/lalopey/opt/anaconda3/envs/tweet-sentiment/lib/python3.7/site-packages/transformers/trainer.py\", line 492, in train\n",
      "    for step, inputs in enumerate(epoch_iterator):\n",
      "  File \"/Users/lalopey/opt/anaconda3/envs/tweet-sentiment/lib/python3.7/site-packages/tqdm/std.py\", line 1129, in __iter__\n",
      "    for obj in iterable:\n",
      "  File \"/Users/lalopey/opt/anaconda3/envs/tweet-sentiment/lib/python3.7/site-packages/torch/utils/data/dataloader.py\", line 346, in __next__\n",
      "    data = self._dataset_fetcher.fetch(index)  # may raise StopIteration\n",
      "  File \"/Users/lalopey/opt/anaconda3/envs/tweet-sentiment/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\", line 47, in fetch\n",
      "    return self.collate_fn(data)\n",
      "  File \"/Users/lalopey/opt/anaconda3/envs/tweet-sentiment/lib/python3.7/site-packages/transformers/data/data_collator.py\", line 87, in __call__\n",
      "    labels[labels == self.tokenizer.pad_token_id] = -100\n",
      "TypeError: eq() received an invalid combination of arguments - got (NoneType), but expected one of:\n",
      " * (Tensor other)\n",
      "      didn't match because some of the arguments have invalid types: (\u001b[31;1mNoneType\u001b[0m)\n",
      " * (Number other)\n",
      "      didn't match because some of the arguments have invalid types: (\u001b[31;1mNoneType\u001b[0m)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!python run_language_modeling.py \\\n",
    "    --output_dir='here/' \\\n",
    "    --model_type=gpt2 \\\n",
    "    --model_name_or_path=gpt2 \\\n",
    "    --do_train \\\n",
    "    --train_data_file='pearkes.txt' \\\n",
    "    --do_eval \\\n",
    "    --eval_data_file='pearkes.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2Model were not initialized from the model checkpoint at gpt2 and are newly initialized: ['h.0.attn.masked_bias', 'h.1.attn.masked_bias', 'h.2.attn.masked_bias', 'h.3.attn.masked_bias', 'h.4.attn.masked_bias', 'h.5.attn.masked_bias', 'h.6.attn.masked_bias', 'h.7.attn.masked_bias', 'h.8.attn.masked_bias', 'h.9.attn.masked_bias', 'h.10.attn.masked_bias', 'h.11.attn.masked_bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2Model\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "model = GPT2Model.from_pretrained(\"gpt2\")\n",
    "\n",
    "%%time\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "data = tokenizer.batch_encode_plus(pearkes.tweet, \n",
    "                            return_tensors=\"pt\",\n",
    "                            padding=True,truncation=True)\n",
    "\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer)\n",
    "\n",
    "from transformers import TextDataset, LineByLineTextDataset\n",
    "a = LineByLineTextDataset(tokenizer=tokenizer, file_path='listfile.txt', block_size=1)\n",
    "\n",
    "from torch.utils.data.dataset import Dataset\n",
    "\n",
    "# define list of places\n",
    "places = pearkes.tweet\n",
    "\n",
    "with open('listfile.txt', 'w') as filehandle:\n",
    "    for listitem in places:\n",
    "        filehandle.write('%s\\n' % listitem)\n",
    "        \n",
    "\n",
    "class GPReviewDataset(Dataset):\n",
    "    def __init__(self, reviews):\n",
    "        self.reviews = reviews\n",
    "    def __len__(self):\n",
    "        return data['input_ids'].shape[0]\n",
    "    def __getitem__(self, item):\n",
    "\n",
    "        return {\n",
    "          'input_ids': self.reviews['input_ids'][item].flatten(),\n",
    "          'attention_mask': self.reviews['attention_mask'][item].flatten(),\n",
    "        }\n",
    "    \n",
    "\n",
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./pearkes\",\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=1,\n",
    "    save_steps=10_000,\n",
    "    save_total_limit=2,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=a,\n",
    "    prediction_loss_only=True,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
