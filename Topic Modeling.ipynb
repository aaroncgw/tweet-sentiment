{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modules.tweet_data import read_raw_data\n",
    "from modules.topics import TopicSeries, display_components\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import spacy\n",
    "import pickle\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading data\n"
     ]
    }
   ],
   "source": [
    "tweet_df = read_raw_data()\n",
    "es = pd.read_csv('data/ES.csv')\n",
    "es = es[es.Time == '15:00'][['Date','Time','Close']]\n",
    "es.Date = pd.to_datetime(es.Date +' ' +'15:45')\n",
    "es.drop('Time',axis=1,inplace=True)\n",
    "es.set_index('Date', inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_range = es['2011-12-30':'2020-5-31'].index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts=TopicSeries()\n",
    "ts.fit(tweet_df, date_range)\n",
    "ts.save('topics' + '-' + str(date_range[0].date()) + '-' + str(date_range[-1].date()) + '.p')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batches = [date_range[-500:], \n",
    "           date_range[-1000:-499], \n",
    "           date_range[-1500:-999], \n",
    "           date_range[:-1499]]\n",
    "\n",
    "for batch in batches:\n",
    "    ts=TopicSeries()\n",
    "    ts.fit(tweet_df, batch)\n",
    "    ts.save('topics' + '-' + str(batch[0].date()) + '-' + str(batch[-1].date()) + '.p')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts = pickle.load(open('data/NMF-2017-03-16-2020-05-29.p','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "classi = pd.read_csv('data/handle_class.csv')\n",
    "tweet_df = pd.merge(tweet_df,classi[['class','handle','subclass']],on=['handle'], right_index=True)\n",
    "# Need to resort by index as merge messes up with datetime indexing\n",
    "tweet_df.sort_index(inplace=True)\n",
    "tweet_df = tweet_df[tweet_df.class == 'fintwit']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>handle</th>\n",
       "      <th>tweet</th>\n",
       "      <th>class</th>\n",
       "      <th>subclass</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>timestamp</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2006-09-22 09:02:46</th>\n",
       "      <td>33743</td>\n",
       "      <td>guan</td>\n",
       "      <td>Hello, World!</td>\n",
       "      <td>fintwit</td>\n",
       "      <td>opinion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2006-09-22 09:21:33</th>\n",
       "      <td>33745</td>\n",
       "      <td>guan</td>\n",
       "      <td>Working on a cool new project at Steffen's house.</td>\n",
       "      <td>fintwit</td>\n",
       "      <td>opinion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2007-03-08 12:50:52</th>\n",
       "      <td>5929557</td>\n",
       "      <td>guan</td>\n",
       "      <td>Working on 23 stuff right now.</td>\n",
       "      <td>fintwit</td>\n",
       "      <td>opinion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2007-04-06 16:29:24</th>\n",
       "      <td>20844701</td>\n",
       "      <td>guan</td>\n",
       "      <td>My new Nokia N95 is fabulous!</td>\n",
       "      <td>fintwit</td>\n",
       "      <td>opinion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2007-04-07 14:01:37</th>\n",
       "      <td>21447221</td>\n",
       "      <td>guan</td>\n",
       "      <td>My Mac is running an option hedging simulation...</td>\n",
       "      <td>fintwit</td>\n",
       "      <td>opinion</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     tweet_id handle  \\\n",
       "timestamp                              \n",
       "2006-09-22 09:02:46     33743   guan   \n",
       "2006-09-22 09:21:33     33745   guan   \n",
       "2007-03-08 12:50:52   5929557   guan   \n",
       "2007-04-06 16:29:24  20844701   guan   \n",
       "2007-04-07 14:01:37  21447221   guan   \n",
       "\n",
       "                                                                 tweet  \\\n",
       "timestamp                                                                \n",
       "2006-09-22 09:02:46                                      Hello, World!   \n",
       "2006-09-22 09:21:33  Working on a cool new project at Steffen's house.   \n",
       "2007-03-08 12:50:52                     Working on 23 stuff right now.   \n",
       "2007-04-06 16:29:24                      My new Nokia N95 is fabulous!   \n",
       "2007-04-07 14:01:37  My Mac is running an option hedging simulation...   \n",
       "\n",
       "                       class subclass  \n",
       "timestamp                              \n",
       "2006-09-22 09:02:46  fintwit  opinion  \n",
       "2006-09-22 09:21:33  fintwit  opinion  \n",
       "2007-03-08 12:50:52  fintwit  opinion  \n",
       "2007-04-06 16:29:24  fintwit  opinion  \n",
       "2007-04-07 14:01:37  fintwit  opinion  "
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on :  2012-02-21\r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-66-f8817a0035c9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mts\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTopicSeries\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtweet_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtweet_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'class'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'fintwit'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdate_range\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'topics'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'-'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdate_range\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'-'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdate_range\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'.p'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/GitHub/tweet-sentiment/modules/topics.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, df, date_range)\u001b[0m\n\u001b[1;32m     84\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Working on : \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr_date\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"\\r\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m             \u001b[0msub_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdate_range\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdate_range\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mdt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimedelta\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseconds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtweet\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcalculate_nmf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr_date\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msub_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcalculate_lda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr_date\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msub_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/GitHub/tweet-sentiment/modules/topics.py\u001b[0m in \u001b[0;36mcalculate_nmf\u001b[0;34m(self, date, data)\u001b[0m\n\u001b[1;32m     46\u001b[0m         tfidf = TfidfVectorizer(stop_words=STOP_WORDS ,\n\u001b[1;32m     47\u001b[0m                                 token_pattern='^[A-Za-z]+|#\\w*[a-zA-Z]+\\w*')\n\u001b[0;32m---> 48\u001b[0;31m         \u001b[0mtfidf_vecs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtfidf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m         \u001b[0;31m# Fit NMF model with n_components topics with TF-IDF input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         nmf = NMF(n_components=self.n_components,\n",
      "\u001b[0;32m~/opt/anaconda3/envs/nlp/lib/python3.7/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   1838\u001b[0m         \"\"\"\n\u001b[1;32m   1839\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1840\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_documents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1841\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tfidf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1842\u001b[0m         \u001b[0;31m# X is already a transformed view of raw_documents so\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/nlp/lib/python3.7/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   1216\u001b[0m                                                        \u001b[0mmax_doc_count\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1217\u001b[0m                                                        \u001b[0mmin_doc_count\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1218\u001b[0;31m                                                        max_features)\n\u001b[0m\u001b[1;32m   1219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1220\u001b[0m             \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sort_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/nlp/lib/python3.7/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36m_limit_features\u001b[0;34m(self, X, vocabulary, high, low, limit)\u001b[0m\n\u001b[1;32m   1078\u001b[0m         \u001b[0mnew_indices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcumsum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m  \u001b[0;31m# maps old indices to new\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1079\u001b[0m         \u001b[0mremoved_terms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1080\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mterm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mold_index\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocabulary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1081\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mold_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1082\u001b[0m                 \u001b[0mvocabulary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mterm\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_indices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mold_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "ts=TopicSeries()\n",
    "ts.fit(tweet_df, date_range)\n",
    "ts.save('topics-fintwit' + '-' + str(date_range[0].date()) + '-' + str(date_range[-1].date()) + '.p')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0:\n",
      "trump like saudi khashoggi people says think good time dead\n",
      "Topic 1:\n",
      "thank weekend great kind fantastic means god nice marc sir\n",
      "Topic 2:\n",
      "china growth gdp trade market economic previous gmt sales expected\n",
      "Topic 3:\n",
      "italy eu budget moscovici says italian bonds commission euro warns\n",
      "Topic 4:\n",
      "traders traded percentage near oct highest gbpusd lowest eurusd oil\n"
     ]
    }
   ],
   "source": [
    "display_components(ts.nmf_dict['2018-10-19'], ts.tfidf_dict['2018-10-19'].get_feature_names(),10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_display = 20\n",
    "top_words = []\n",
    "word_features =  ts.tfidf_dict['2018-10-19'].get_feature_names()\n",
    "for topic_idx, topic in enumerate(ts.nmf_dict['2018-10-19'].components_):\n",
    "    top_words_idx = topic.argsort()[::-1][:top_display]\n",
    "    top_words.append([word_features[i] for i in top_words_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "107.09420242775161"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ts.nmf_dict['2018-10-19'].reconstruction_err_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Measures between topics and days:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusion: cosine similarity between top words should work\n",
    "\n",
    "Cross entropy could work for LDA if probabilities. Take top XX words from each topic, and calculate\n",
    "cross entropy with next day. You need to concat the words and find the index for each word in each day\n",
    "\n",
    "Are NMF vectors probabilities?\n",
    "\n",
    "Topics  \n",
    "NMF  \n",
    "Calc:   \n",
    "recon error  \n",
    "topic coherence (intraday) if high average, probably less topics leading, if low, more topics  \n",
    "between days distances  \n",
    "Avg of max  If low, change in subjects  \n",
    "num Of unique topics   If low, change in subjects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cosine Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['trump',\n",
       " 'like',\n",
       " 'saudi',\n",
       " 'khashoggi',\n",
       " 'people',\n",
       " 'says',\n",
       " 'think',\n",
       " 'good',\n",
       " 'time',\n",
       " 'dead',\n",
       " 'looks',\n",
       " 'know',\n",
       " 'jamal',\n",
       " 'arabia',\n",
       " 'president',\n",
       " 'certainly',\n",
       " 'going',\n",
       " 'journalist',\n",
       " 'way',\n",
       " 'right']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_words[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp_mid = spacy.load('en_core_web_md')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.0000001 , 0.8195414 , 0.6415489 , 0.6218434 , 0.50415957],\n",
       "       [0.8195414 , 0.9999999 , 0.5254425 , 0.54699445, 0.46578452],\n",
       "       [0.6415489 , 0.5254425 , 0.99999964, 0.7501086 , 0.706774  ],\n",
       "       [0.6218434 , 0.54699445, 0.7501086 , 1.0000002 , 0.64579254],\n",
       "       [0.50415957, 0.46578452, 0.706774  , 0.64579254, 1.0000001 ]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarity([nlp_mid(' '.join(top_words[i])).vector for i in range(len(top_words))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatetimeIndex(['2020-05-18 15:45:00', '2020-05-19 15:45:00',\n",
       "               '2020-05-20 15:45:00', '2020-05-21 15:45:00'],\n",
       "              dtype='datetime64[ns]', name='Date', freq=None)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates = ['2020-05-19', '2020-05-20', '2020-05-21']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'2020-05-19': NMF(n_components=5, random_state=42),\n",
       " '2020-05-20': NMF(n_components=5, random_state=42),\n",
       " '2020-05-21': NMF(n_components=5, random_state=42)}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ts.nmf_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_words_list = []\n",
    "top_args_list = []\n",
    "top_display = 20\n",
    "\n",
    "for date in dates:\n",
    "    top_words = []\n",
    "    top_args = []\n",
    "    word_features =  ts.tfidf_dict[date].get_feature_names()\n",
    "    for topic_idx, topic in enumerate(ts.nmf_dict[date].components_):\n",
    "        top_words_idx = topic.argsort()[::-1][:top_display]\n",
    "        top_words.append([word_features[i] for i in top_words_idx])\n",
    "        top_args.append(top_words_idx)\n",
    "    top_words_list.append(top_words)\n",
    "    top_args_list.append(top_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "concat_words = [[' '.join(top_words) for top_words in sub_top_list] for sub_top_list in top_words_list]\n",
    "word_vectors = [[nlp_mid(topic_words).vector for topic_words in concat_word] for concat_word in concat_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_coherence = []\n",
    "topic_coherence_diff = []\n",
    "num_topics_change = []\n",
    "\n",
    "n_topics = 5\n",
    "den = (n_topics**2-n_topics)\n",
    "for i in range(len(word_vectors)-1):\n",
    "    cs = cosine_similarity(word_vectors[i] + word_vectors[i+1])\n",
    "    # Topic coherence\n",
    "    tc = (cs[0:n_topics,0:n_topics].sum() - cs[0:n_topics,0:n_topics].diagonal().sum())/den\n",
    "    topic_coherence.append(tc)\n",
    "    # Topic coherence difference betweent day i and i+1\n",
    "    tcd = cs[0:n_topics,n_topics:2*n_topics].max(axis=1).mean()\n",
    "    topic_coherence_diff.append(tcd) \n",
    "    num_ts = len(np.unique(cs[0:n_topics,n_topics:2*n_topics].argmax(axis=1)))\n",
    "    num_topics_change.append(num_ts)\n",
    "tc = (cs[n_topics:2*n_topics,n_topics:2*n_topics].sum() \\\n",
    "      - cs[n_topics:2*n_topics,n_topics:2*n_topics].diagonal().sum())/den\n",
    "topic_coherence.append(tc)\n",
    "  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([10657, 17005, 17100,  4928, 17145, 18839,  7623, 14520,  8009,\n",
       "         3695, 12601,  7819, 11632, 16256,  9813,  2635, 18841, 16262,\n",
       "        18438, 17276]),\n",
       " array([10193, 10364, 10359,  6946, 17560, 15870, 18838, 12601, 14520,\n",
       "        14781,  1265,  9813, 15866,  9194,  8606, 10800, 10395,  1606,\n",
       "         3417, 12006]),\n",
       " array([ 7836, 12932, 11640, 11298, 16964, 17106, 10437, 17025, 17005,\n",
       "        17542,  9813, 11998,  6508,  9765, 12832, 15870,  2277, 16379,\n",
       "         3436,  8191]),\n",
       " array([17458,  7196, 16604, 12330, 13224, 12331, 15178,  2832, 14921,\n",
       "         4671, 10574, 11632,  2850,  1288,  2825,  4285,  2992, 11640,\n",
       "         6903, 10526]),\n",
       " array([ 1664,  1606,  6004,  7846,  1573, 14442, 15297, 11131, 14830,\n",
       "        18300, 14385,  6464,  7845,  3893, 16318,  9159, 10277,  8606,\n",
       "        11133, 18860])]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[9.41691146e-01, 7.29245633e-01, 7.22425772e-01, 5.82367276e-01,\n",
       "        5.40866538e-01, 4.67197163e-01, 4.54668552e-01, 4.54618090e-01,\n",
       "        4.44260666e-01, 4.39739661e-01, 4.35161474e-01, 3.99852181e-01,\n",
       "        3.91100571e-01, 3.79672436e-01, 3.39545932e-01, 3.29667052e-01,\n",
       "        3.24333526e-01, 3.14946909e-01, 2.97518570e-01, 2.90099951e-01],\n",
       "       [8.33436601e-03, 6.34034190e-04, 0.00000000e+00, 3.16908490e-02,\n",
       "        3.24202594e-03, 0.00000000e+00, 0.00000000e+00, 6.08975172e-02,\n",
       "        0.00000000e+00, 0.00000000e+00, 6.35425280e-02, 1.98853321e-02,\n",
       "        0.00000000e+00, 0.00000000e+00, 4.76677619e-02, 1.03352506e-02,\n",
       "        1.34543845e-02, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "       [0.00000000e+00, 9.46384721e-02, 1.78969527e-02, 4.06346078e-02,\n",
       "        0.00000000e+00, 0.00000000e+00, 2.94953183e-03, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 8.17845204e-02, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 2.53470480e-03, 2.09714664e-02],\n",
       "       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        2.57222294e-03, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 6.03880704e-03, 6.76857661e-02, 0.00000000e+00,\n",
       "        3.61418063e-01, 0.00000000e+00, 4.35556536e-02, 2.02167316e-03,\n",
       "        0.00000000e+00, 0.00000000e+00, 2.79254688e-03, 2.44669469e-02],\n",
       "       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        5.91170221e-02, 4.35610507e-03, 0.00000000e+00, 0.00000000e+00,\n",
       "        6.41227353e-02, 0.00000000e+00, 0.00000000e+00, 2.98753780e-02,\n",
       "        0.00000000e+00, 4.62216543e-02, 0.00000000e+00, 4.71838759e-02,\n",
       "        0.00000000e+00, 4.94406583e-02, 0.00000000e+00, 2.43217217e-02]])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 5x20, each of these is a probability, how to use entropy to measure?\n",
    "ts.nmf_dict[date].components_[:,top_args[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need to match word probabilities/indices between days\n",
    "aaa = ts.nmf_dict[date].components_[0]/ts.nmf_dict[date].components_[0].sum()\n",
    "bbb = ts.nmf_dict[date].components_[1]/ts.nmf_dict[date].components_[1].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "shapes (17844,) and (5872,) not aligned: 17844 (dim 0) != 5872 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-105-f0d4098d7cf6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0maaa\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maaa\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbbb\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbbb\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m: shapes (17844,) and (5872,) not aligned: 17844 (dim 0) != 5872 (dim 0)"
     ]
    }
   ],
   "source": [
    "aaa[aaa>0].dot(np.log(bbb[bbb>0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.03869419e-02, 3.19901698e-02, 5.92124026e-04, ...,\n",
       "        1.72551758e-04, 3.35822443e-04, 1.72551758e-04],\n",
       "       [1.00000000e-15, 1.00000000e-15, 1.00000000e-15, ...,\n",
       "        1.00000000e-15, 1.00000000e-15, 1.00000000e-15],\n",
       "       [7.67170612e-03, 1.00000000e-15, 1.00000000e-15, ...,\n",
       "        1.00000000e-15, 1.00000000e-15, 1.00000000e-15],\n",
       "       [1.02803427e-03, 2.37004812e-02, 1.00000000e-15, ...,\n",
       "        5.28806240e-05, 1.00000000e-15, 5.28806240e-05],\n",
       "       [1.00000000e-15, 1.00000000e-15, 1.40111904e-04, ...,\n",
       "        1.00000000e-15, 1.00000000e-15, 1.00000000e-15]])"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = ts.nmf_dict[date].components_\n",
    "A[A==0] = 1e-15\n",
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8.375995828555052"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entropy(ts.nmf_dict[date].components_[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17.086682064097044"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entropy(ts.nmf_dict[date].components_[0],qk=(ts.nmf_dict[date].components_[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0.0, inf, inf, inf, inf],\n",
       " [inf, 0.0, inf, inf, inf],\n",
       " [inf, inf, 0.0, inf, inf],\n",
       " [inf, inf, inf, 0.0, inf],\n",
       " [inf, inf, inf, inf, 0.0]]"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[[entropy(ts.nmf_dict[dates[0]].components_[i],qk=(ts.nmf_dict[dates[0]].components_[j])) for i in range(5)]for j in range(5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[[entropy(ts.nmf_dict[date].components_[i],qk=(ts.nmf_dict[date].components_[j])) for i in range(5)]for j in range(5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18.70549419755346"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entropy(ts.nmf_dict[date].components_[0],qk=(ts.nmf_dict[date].components_[2]+1e-15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15.274803048704879"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entropy(ts.nmf_dict[date].components_[0],qk=(ts.nmf_dict[date].components_[3]+1e-15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19.802016016333614"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entropy(ts.nmf_dict[date].components_[0],qk=(ts.nmf_dict[date].components_[4]+1e-15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lalopey/opt/anaconda3/envs/tweet-sentiment/lib/python3.7/site-packages/ipykernel_launcher.py:1: RuntimeWarning: divide by zero encountered in log\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "nan"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ts.nmf_dict[date].components_[0].dot(np.log(ts.nmf_dict[date].components_[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14.961622403118877"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ts.nmf_dict[date].components_[1].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-228.81431983349367"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ts.nmf_dict[date].components_[0][ts.nmf_dict[date].components_[0] > 0.01].dot(np.log(ts.nmf_dict[date].components_[0][ts.nmf_dict[date].components_[0] > 0.01]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lalopey/opt/anaconda3/envs/tweet-sentiment/lib/python3.7/site-packages/ipykernel_launcher.py:1: RuntimeWarning: divide by zero encountered in log\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "/Users/lalopey/opt/anaconda3/envs/tweet-sentiment/lib/python3.7/site-packages/ipykernel_launcher.py:1: RuntimeWarning: invalid value encountered in multiply\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([-0.0474393 , -0.11012061, -0.00440054, ..., -0.00149513,\n",
       "       -0.00268622, -0.00149513])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ts.nmf_dict[date].components_[0] * np.log(ts.nmf_dict[date].components_[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "q = [1]\n",
    "if q:\n",
    "    print(1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
